{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f91ddfc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nltk in c:\\programdata\\anaconda3\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\programdata\\anaconda3\\lib\\site-packages (1.4.2)\n",
      "Requirement already satisfied: gensim in c:\\users\\attar\\appdata\\roaming\\python\\python312\\site-packages (4.3.3)\n",
      "Requirement already satisfied: click in c:\\users\\attar\\appdata\\roaming\\python\\python312\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\attar\\appdata\\roaming\\python\\python312\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (4.66.4)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\attar\\appdata\\roaming\\python\\python312\\site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from gensim) (5.2.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\attar\\appdata\\roaming\\python\\python312\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk scikit-learn gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94cb352c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\attar/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3842387b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer , TfidfVectorizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models import word2vec\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0229859",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    \"Natural language processing is fascinating.\",\n",
    "    \"Language processing includes syntax and semantics.\",\n",
    "    \"Text data needs preprocessing for NLP models.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0257cc02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['natural', 'language', 'processing', 'is', 'fascinating', '.'],\n",
       " ['language', 'processing', 'includes', 'syntax', 'and', 'semantics', '.'],\n",
       " ['text', 'data', 'needs', 'preprocessing', 'for', 'nlp', 'models', '.']]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_docs = [word_tokenize(doc.lower()) for doc in documents]\n",
    "tokenized_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e39cd89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. Bag of Words - CountVectorizer:\n",
      "  (0, 8)\t1\n",
      "  (0, 6)\t1\n",
      "  (0, 12)\t1\n",
      "  (0, 5)\t1\n",
      "  (0, 2)\t1\n",
      "  (1, 6)\t1\n",
      "  (1, 12)\t1\n",
      "  (1, 4)\t1\n",
      "  (1, 14)\t1\n",
      "  (1, 0)\t1\n",
      "  (1, 13)\t1\n",
      "  (2, 15)\t1\n",
      "  (2, 1)\t1\n",
      "  (2, 9)\t1\n",
      "  (2, 11)\t1\n",
      "  (2, 3)\t1\n",
      "  (2, 10)\t1\n",
      "  (2, 7)\t1\n"
     ]
    }
   ],
   "source": [
    "# 1. BAG OF WORDS - RAW COUNTS\n",
    "print(\"\\n1. Bag of Words - CountVectorizer:\")\n",
    "count_vectorizer = CountVectorizer()\n",
    "bow_count = count_vectorizer.fit_transform(documents)\n",
    "print(bow_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8b5986c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: {'natural': 8, 'language': 6, 'processing': 12, 'is': 5, 'fascinating': 2, 'includes': 4, 'syntax': 14, 'and': 0, 'semantics': 13, 'text': 15, 'data': 1, 'needs': 9, 'preprocessing': 11, 'for': 3, 'nlp': 10, 'models': 7}\n",
      "BoW Matrix (Raw Counts):\n",
      " [[0 0 1 0 0 1 1 0 1 0 0 0 1 0 0 0]\n",
      " [1 0 0 0 1 0 1 0 0 0 0 0 1 1 1 0]\n",
      " [0 1 0 1 0 0 0 1 0 1 1 1 0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocabulary:\", count_vectorizer.vocabulary_)\n",
    "print(\"BoW Matrix (Raw Counts):\\n\", bow_count.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67ca438c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2. Normalized Count (Term Frequency):\n",
      "BoW Matrix (Normalized):\n",
      " [[0.         0.         0.2        0.         0.         0.2\n",
      "  0.2        0.         0.2        0.         0.         0.\n",
      "  0.2        0.         0.         0.        ]\n",
      " [0.16666667 0.         0.         0.         0.16666667 0.\n",
      "  0.16666667 0.         0.         0.         0.         0.\n",
      "  0.16666667 0.16666667 0.16666667 0.        ]\n",
      " [0.         0.14285714 0.         0.14285714 0.         0.\n",
      "  0.         0.14285714 0.         0.14285714 0.14285714 0.14285714\n",
      "  0.         0.         0.         0.14285714]]\n"
     ]
    }
   ],
   "source": [
    "# 2. BAG OF WORDS - NORMALIZED COUNTS (Term Frequency)\n",
    "print(\"\\n2. Normalized Count (Term Frequency):\")\n",
    "normalized_bow = bow_count.toarray().astype(float)\n",
    "normalized_bow = normalized_bow / normalized_bow.sum(axis=1, keepdims=True)\n",
    "print(\"BoW Matrix (Normalized):\\n\", normalized_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d119451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3. TF-IDF Vectorizer:\n",
      "Vocabulary {'natural': 8, 'language': 6, 'processing': 12, 'is': 5, 'fascinating': 2, 'includes': 4, 'syntax': 14, 'and': 0, 'semantics': 13, 'text': 15, 'data': 1, 'needs': 9, 'preprocessing': 11, 'for': 3, 'nlp': 10, 'models': 7}\n",
      "Matrix  [[0.         0.         0.49047908 0.         0.         0.49047908\n",
      "  0.37302199 0.         0.49047908 0.         0.         0.\n",
      "  0.37302199 0.         0.         0.        ]\n",
      " [0.44036207 0.         0.         0.         0.44036207 0.\n",
      "  0.3349067  0.         0.         0.         0.         0.\n",
      "  0.3349067  0.44036207 0.44036207 0.        ]\n",
      " [0.         0.37796447 0.         0.37796447 0.         0.\n",
      "  0.         0.37796447 0.         0.37796447 0.37796447 0.37796447\n",
      "  0.         0.         0.         0.37796447]]\n"
     ]
    }
   ],
   "source": [
    "# 3. TF-IDF\n",
    "print(\"\\n3. TF-IDF Vectorizer:\")\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
    "print(\"Vocabulary\" , tfidf_vectorizer.vocabulary_)\n",
    "print(\"Matrix \", tfidf_matrix.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "34acfc45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "4. Word2Vec Embeddings:\n"
     ]
    }
   ],
   "source": [
    "# 4. WORD2VEC EMBEDDINGS\n",
    "print(\"\\n4. Word2Vec Embeddings:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d2376af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e631f1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = Word2Vec(sentences=tokenized_docs, vector_size=50, window=2, min_count=1, workers=1, sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9f18155b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding for 'language':\n",
      " [-0.01631583  0.0089916  -0.00827415  0.00164907  0.01699724 -0.00892435\n",
      "  0.009035   -0.01357392 -0.00709698  0.01879702 -0.00315531  0.00064274\n",
      " -0.00828126 -0.01536538 -0.00301602  0.00493959 -0.00177605  0.01106732\n",
      " -0.00548595  0.00452013  0.01091159  0.01669191 -0.00290748 -0.01841629\n",
      "  0.0087411   0.00114357  0.01488382 -0.00162657 -0.00527683 -0.01750602\n",
      " -0.00171311  0.00565313  0.01080286  0.01410531 -0.01140624  0.00371764\n",
      "  0.01217773 -0.0095961  -0.00621452  0.01359526  0.00326295  0.00037983\n",
      "  0.00694727  0.00043555  0.01923765  0.01012121 -0.01783478 -0.01408312\n",
      "  0.00180291  0.01278507]\n"
     ]
    }
   ],
   "source": [
    "word = \"language\"\n",
    "if word in w2v_model.wv:\n",
    "    print(f\"Embedding for '{word}':\\n\", w2v_model.wv[word])\n",
    "else:\n",
    "    print(f\"'{word}' not in vocabulary.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8e47e884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Word2Vec per Document:\n",
      "Doc 1 vector (shape (50,)):\n",
      " [-0.01123268  0.00799814  0.00101659  0.00501263  0.00292931 -0.00828414\n",
      "  0.0075939   0.00214159 -0.00620084 -0.00251304  0.00759771 -0.00477096\n",
      "  0.0025906  -0.00076397  0.00258381  0.0053326   0.00630225  0.00407015\n",
      " -0.01239089  0.00063905  0.00727931  0.00712224  0.01227268 -0.00448857\n",
      "  0.00267234  0.00440516  0.0049474   0.00259646 -0.00389557 -0.0045686\n",
      "  0.00326978 -0.00684832  0.00150382  0.00051723  0.00027171  0.00343717\n",
      "  0.00792728 -0.0035297   0.00311415  0.00406422 -0.00118404  0.00423477\n",
      "  0.00030699 -0.00421173  0.00690528  0.00639761 -0.00042833  0.00110828\n",
      "  0.00274217  0.00139764]\n",
      "Doc 2 vector (shape (50,)):\n",
      " [-8.2321512e-03  4.2626252e-03 -5.2243657e-03 -2.0097645e-03\n",
      "  7.0426852e-04 -1.7454776e-03  4.5792903e-03  7.4597066e-03\n",
      " -7.0715835e-03  4.9250026e-04 -2.9334484e-03 -1.5505851e-03\n",
      " -3.6388363e-03 -5.3067890e-04  3.0730970e-04 -4.7470122e-03\n",
      "  1.1409502e-03  9.1627985e-03 -6.3393340e-03 -8.3525945e-03\n",
      "  1.6125755e-03 -2.6058033e-03  7.0441379e-03  2.6166206e-05\n",
      "  3.7821899e-03 -3.5388898e-03  2.4364742e-03  5.0022402e-03\n",
      " -8.9387055e-03  2.0569046e-03  4.1208132e-03 -2.9458650e-03\n",
      "  2.2548751e-03 -6.6552572e-03 -1.6508152e-03  3.9222275e-04\n",
      "  9.3254754e-03  3.5579782e-03  2.8689445e-03 -2.4844825e-04\n",
      " -5.0198287e-04 -4.3760356e-03 -9.4669042e-03  1.8507127e-03\n",
      "  7.3029781e-03  1.8846364e-03 -1.4740961e-04 -4.0791981e-04\n",
      "  1.3086463e-03  5.9725292e-04]\n",
      "Doc 3 vector (shape (50,)):\n",
      " [ 0.00535078 -0.00811982  0.00787253  0.00526222 -0.00985717 -0.00048003\n",
      "  0.00368466 -0.00250126 -0.00723199 -0.0050185   0.00633433 -0.00284523\n",
      "  0.0022187   0.0028344  -0.00181778  0.00480102  0.00498574  0.00458928\n",
      " -0.00654852 -0.00791097  0.00066734  0.00470659  0.00746325  0.00283178\n",
      "  0.00103161  0.00221472 -0.00310889  0.00446343 -0.00031482 -0.00204673\n",
      " -0.00451729 -0.00154063  0.00141264 -0.00613644 -0.00457183 -0.00717312\n",
      "  0.00336044 -0.00245529 -0.00500655 -0.00492451  0.00484992 -0.00101499\n",
      " -0.0034948  -0.00434722  0.00480254  0.00083228 -0.00148225 -0.00495315\n",
      "  0.00161133  0.00239196]\n"
     ]
    }
   ],
   "source": [
    "# Average Word2Vec for each sentence\n",
    "print(\"\\nAverage Word2Vec per Document:\")\n",
    "def document_vector(doc):\n",
    "    doc = [word for word in doc if word in w2v_model.wv]\n",
    "    if len(doc) == 0:\n",
    "        return np.zeros(w2v_model.vector_size)\n",
    "    return np.mean(w2v_model.wv[doc], axis=0)\n",
    "\n",
    "avg_vectors = [document_vector(doc) for doc in tokenized_docs]\n",
    "for i, vec in enumerate(avg_vectors):\n",
    "    print(f\"Doc {i+1} vector (shape {vec.shape}):\\n\", vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1557ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
